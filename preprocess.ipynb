{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn plotly scipy scikit-learn pythainlp folium tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thai NLP\n",
    "from pythainlp import word_tokenize\n",
    "from pythainlp.util import normalize as thai_normalize\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('bangkok_traffy.csv')\n",
    "print(f\"Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Overview\n",
    "print(\"=\"*60)\n",
    "print(\"DATA INFO\")\n",
    "print(\"=\"*60)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NUMERICAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Analysis\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing Count': missing.values,\n",
    "    'Missing %': missing_pct.values\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Visualize missing values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of missing percentages\n",
    "colors = ['#e74c3c' if x > 0 else '#27ae60' for x in missing_pct.values]\n",
    "axes[0].barh(missing.index, missing_pct.values, color=colors)\n",
    "axes[0].set_xlabel('Missing Percentage (%)')\n",
    "axes[0].set_title('Missing Values by Column')\n",
    "axes[0].axvline(x=50, color='red', linestyle='--', alpha=0.5, label='50% threshold')\n",
    "\n",
    "# Heatmap of missing pattern (sample for performance)\n",
    "sample_df = df.sample(min(1000, len(df)), random_state=42)\n",
    "sns.heatmap(sample_df.isnull(), cbar=True, yticklabels=False, ax=axes[1], cmap='YlOrRd')\n",
    "axes[1].set_title('Missing Value Pattern (Sample)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Type Conversions\n",
    "print(\"Converting data types...\")\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "df['last_activity'] = pd.to_datetime(df['last_activity'], errors='coerce')\n",
    "\n",
    "# Parse coordinates into latitude and longitude\n",
    "def parse_coords(coord_str):\n",
    "    \"\"\"Parse 'lon,lat' string into separate values\"\"\"\n",
    "    if pd.isna(coord_str):\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        parts = str(coord_str).split(',')\n",
    "        if len(parts) == 2:\n",
    "            lon, lat = float(parts[0]), float(parts[1])\n",
    "            return lat, lon\n",
    "    except:\n",
    "        pass\n",
    "    return np.nan, np.nan\n",
    "\n",
    "coords_parsed = df['coords'].apply(parse_coords)\n",
    "df['latitude'] = coords_parsed.apply(lambda x: x[0])\n",
    "df['longitude'] = coords_parsed.apply(lambda x: x[1])\n",
    "\n",
    "# Parse type column (remove braces)\n",
    "def parse_type(type_str):\n",
    "    \"\"\"Parse '{cat1,cat2}' into list of categories\"\"\"\n",
    "    if pd.isna(type_str):\n",
    "        return []\n",
    "    # Remove braces and split\n",
    "    cleaned = str(type_str).strip('{}')\n",
    "    return [t.strip() for t in cleaned.split(',') if t.strip()]\n",
    "\n",
    "df['type_list'] = df['type'].apply(parse_type)\n",
    "df['num_categories'] = df['type_list'].apply(len)\n",
    "\n",
    "# Convert numeric columns\n",
    "df['star'] = pd.to_numeric(df['star'], errors='coerce')\n",
    "df['count_reopen'] = pd.to_numeric(df['count_reopen'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "print(\"Data types after conversion:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nNew columns added: latitude, longitude, type_list, num_categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Temporal Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"TEMPORAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract temporal features\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Complaints over time (monthly)\n",
    "monthly = df.groupby(df['timestamp'].dt.to_period('M')).size()\n",
    "axes[0, 0].plot(monthly.index.astype(str), monthly.values, marker='o', markersize=3)\n",
    "axes[0, 0].set_title('Complaints Over Time (Monthly)')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Number of Complaints')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "# Show only every nth label for readability\n",
    "n = max(1, len(monthly) // 12)\n",
    "for i, label in enumerate(axes[0, 0].xaxis.get_ticklabels()):\n",
    "    if i % n != 0:\n",
    "        label.set_visible(False)\n",
    "\n",
    "# Complaints by year\n",
    "yearly = df['year'].value_counts().sort_index()\n",
    "axes[0, 1].bar(yearly.index.astype(str), yearly.values, color='steelblue')\n",
    "axes[0, 1].set_title('Complaints by Year')\n",
    "axes[0, 1].set_xlabel('Year')\n",
    "axes[0, 1].set_ylabel('Number of Complaints')\n",
    "\n",
    "# Complaints by day of week\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "dow_counts = df['day_of_week'].value_counts().sort_index()\n",
    "axes[1, 0].bar(dow_names, dow_counts.values, color='coral')\n",
    "axes[1, 0].set_title('Complaints by Day of Week')\n",
    "axes[1, 0].set_xlabel('Day')\n",
    "axes[1, 0].set_ylabel('Number of Complaints')\n",
    "\n",
    "# Complaints by hour\n",
    "hourly = df['hour'].value_counts().sort_index()\n",
    "axes[1, 1].bar(hourly.index, hourly.values, color='seagreen')\n",
    "axes[1, 1].set_title('Complaints by Hour of Day')\n",
    "axes[1, 1].set_xlabel('Hour')\n",
    "axes[1, 1].set_ylabel('Number of Complaints')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Geographic Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"GEOGRAPHIC ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Top districts by complaints\n",
    "district_counts = df['district'].value_counts().head(20)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart of top districts\n",
    "axes[0].barh(district_counts.index[::-1], district_counts.values[::-1], color='teal')\n",
    "axes[0].set_xlabel('Number of Complaints')\n",
    "axes[0].set_title('Top 20 Districts by Complaints')\n",
    "\n",
    "# Coordinate distribution\n",
    "valid_coords = df[(df['latitude'].notna()) & (df['longitude'].notna())]\n",
    "print(f\"Valid coordinates: {len(valid_coords):,} ({len(valid_coords)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Scatter plot of coordinates (sample for performance)\n",
    "sample_coords = valid_coords.sample(min(10000, len(valid_coords)), random_state=42)\n",
    "axes[1].scatter(sample_coords['longitude'], sample_coords['latitude'], alpha=0.3, s=1)\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "axes[1].set_title('Complaint Locations (Sample)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interactive map with Plotly\n",
    "print(\"\\nInteractive Map (Density):\")\n",
    "fig_map = px.density_mapbox(\n",
    "    sample_coords,\n",
    "    lat='latitude',\n",
    "    lon='longitude',\n",
    "    radius=5,\n",
    "    center=dict(lat=13.75, lon=100.5),\n",
    "    zoom=10,\n",
    "    mapbox_style='carto-positron',\n",
    "    title='Bangkok Complaint Density Map'\n",
    ")\n",
    "fig_map.update_layout(height=500)\n",
    "fig_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Category Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"CATEGORY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Flatten all categories\n",
    "all_categories = [cat for cats in df['type_list'] for cat in cats]\n",
    "category_counts = Counter(all_categories)\n",
    "top_categories = pd.DataFrame(category_counts.most_common(15), columns=['Category', 'Count'])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top complaint types\n",
    "axes[0, 0].barh(top_categories['Category'][::-1], top_categories['Count'][::-1], color='purple')\n",
    "axes[0, 0].set_xlabel('Count')\n",
    "axes[0, 0].set_title('Top 15 Complaint Categories')\n",
    "\n",
    "# Status distribution\n",
    "status_counts = df['state'].value_counts()\n",
    "axes[0, 1].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', \n",
    "               colors=sns.color_palette('Set2'))\n",
    "axes[0, 1].set_title('Complaint Status Distribution')\n",
    "\n",
    "# Star rating distribution\n",
    "star_counts = df['star'].value_counts().sort_index()\n",
    "axes[1, 0].bar(star_counts.index.astype(str), star_counts.values, color='gold')\n",
    "axes[1, 0].set_xlabel('Star Rating')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Star Rating Distribution')\n",
    "\n",
    "# Number of categories per complaint\n",
    "cat_per_complaint = df['num_categories'].value_counts().sort_index()\n",
    "axes[1, 1].bar(cat_per_complaint.index, cat_per_complaint.values, color='salmon')\n",
    "axes[1, 1].set_xlabel('Number of Categories')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Categories per Complaint')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top organizations\n",
    "print(\"\\nTop 10 Organizations by Workload:\")\n",
    "org_counts = df['organization'].value_counts().head(10)\n",
    "print(org_counts.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Text Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comment length analysis\n",
    "df['comment_length'] = df['comment'].fillna('').apply(len)\n",
    "df['comment_word_count'] = df['comment'].fillna('').apply(lambda x: len(str(x).split()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Comment length distribution\n",
    "axes[0].hist(df['comment_length'], bins=50, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Comment Length Distribution')\n",
    "axes[0].axvline(df['comment_length'].median(), color='red', linestyle='--', label=f'Median: {df[\"comment_length\"].median():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(df['comment_word_count'], bins=50, color='coral', edgecolor='white', alpha=0.7)\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Comment Word Count Distribution')\n",
    "axes[1].axvline(df['comment_word_count'].median(), color='red', linestyle='--', label=f'Median: {df[\"comment_word_count\"].median():.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Thai word frequency analysis (sample for performance)\n",
    "print(\"\\nAnalyzing Thai text (sample of 10,000 comments)...\")\n",
    "sample_comments = df['comment'].dropna().sample(min(10000, len(df['comment'].dropna())), random_state=42)\n",
    "\n",
    "# Tokenize and count words\n",
    "all_words = []\n",
    "for comment in sample_comments:\n",
    "    try:\n",
    "        tokens = word_tokenize(str(comment), engine='newmm')\n",
    "        # Filter out short tokens and common stopwords\n",
    "        tokens = [t for t in tokens if len(t) > 1 and t not in [' ', '\\n', '\\t', 'ๆ', 'ที่', 'และ', 'ของ', 'ใน', 'มี', 'เป็น', 'ได้', 'จะ', 'ให้', 'กับ']]\n",
    "        all_words.extend(tokens)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "top_words = pd.DataFrame(word_freq.most_common(20), columns=['Word', 'Count'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(top_words['Word'][::-1], top_words['Count'][::-1], color='purple')\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 Most Common Words in Comments (excluding stopwords)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal unique words: {len(word_freq):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Correlation Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate resolution time\n",
    "df['resolution_time_hours'] = (df['last_activity'] - df['timestamp']).dt.total_seconds() / 3600\n",
    "\n",
    "# Select numeric columns for correlation\n",
    "numeric_cols = ['star', 'count_reopen', 'num_categories', 'comment_length', \n",
    "                'latitude', 'longitude', 'year', 'month', 'day_of_week', 'hour',\n",
    "                'resolution_time_hours']\n",
    "numeric_df = df[numeric_cols].dropna()\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, square=True, linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resolution time by status\n",
    "print(\"\\nResolution Time by Status:\")\n",
    "resolution_by_status = df.groupby('state')['resolution_time_hours'].agg(['mean', 'median', 'count'])\n",
    "print(resolution_by_status.round(2))\n",
    "\n",
    "# Resolution time by category (top 10)\n",
    "print(\"\\nMean Resolution Time by Category (Top 10 categories):\")\n",
    "# Explode type_list to get resolution time per category\n",
    "df_exploded = df.explode('type_list')\n",
    "resolution_by_cat = df_exploded.groupby('type_list')['resolution_time_hours'].mean().sort_values(ascending=False).head(10)\n",
    "print(resolution_by_cat.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING: Handle Missing Values\n",
    "print(\"=\"*60)\n",
    "print(\"DATA CLEANING: MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store original shape\n",
    "original_shape = df.shape\n",
    "print(f\"Original dataset shape: {original_shape}\")\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1. Handle photo columns - create binary flags\n",
    "df_clean['has_photo'] = df_clean['photo'].notna().astype(int)\n",
    "df_clean['has_photo_after'] = df_clean['photo_after'].notna().astype(int)\n",
    "\n",
    "# 2. Handle comment - fill with empty string\n",
    "df_clean['comment'] = df_clean['comment'].fillna('')\n",
    "\n",
    "# 3. Handle coordinates - keep rows with valid coords for geo analysis\n",
    "# Flag rows with missing coordinates instead of dropping\n",
    "df_clean['has_valid_coords'] = (df_clean['latitude'].notna() & df_clean['longitude'].notna()).astype(int)\n",
    "\n",
    "# 4. Handle star rating - fill with 0 (unknown/not rated)\n",
    "df_clean['star'] = df_clean['star'].fillna(0).astype(int)\n",
    "\n",
    "# 5. Handle address fields - fill with 'Unknown'\n",
    "df_clean['address'] = df_clean['address'].fillna('Unknown')\n",
    "df_clean['subdistrict'] = df_clean['subdistrict'].fillna('Unknown')\n",
    "df_clean['district'] = df_clean['district'].fillna('Unknown')\n",
    "\n",
    "# 6. Handle organization - fill with 'Unknown'\n",
    "df_clean['organization'] = df_clean['organization'].fillna('Unknown')\n",
    "\n",
    "# Report cleaning results\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "missing_after = df_clean.isnull().sum()\n",
    "print(missing_after[missing_after > 0] if missing_after.sum() > 0 else \"No missing values!\")\n",
    "\n",
    "print(f\"\\nNew binary features created: has_photo, has_photo_after, has_valid_coords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING: Clean Coordinates\n",
    "print(\"=\"*60)\n",
    "print(\"DATA CLEANING: COORDINATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Bangkok bounding box (approximate)\n",
    "BANGKOK_LAT_MIN, BANGKOK_LAT_MAX = 13.5, 14.0\n",
    "BANGKOK_LON_MIN, BANGKOK_LON_MAX = 100.3, 100.9\n",
    "\n",
    "# Check coordinate validity\n",
    "valid_mask = (\n",
    "    (df_clean['latitude'] >= BANGKOK_LAT_MIN) & \n",
    "    (df_clean['latitude'] <= BANGKOK_LAT_MAX) &\n",
    "    (df_clean['longitude'] >= BANGKOK_LON_MIN) & \n",
    "    (df_clean['longitude'] <= BANGKOK_LON_MAX)\n",
    ")\n",
    "\n",
    "# Create validity flag\n",
    "df_clean['coords_in_bangkok'] = valid_mask.astype(int)\n",
    "\n",
    "# Report\n",
    "total_with_coords = df_clean['has_valid_coords'].sum()\n",
    "valid_bangkok = valid_mask.sum()\n",
    "outliers = total_with_coords - valid_bangkok\n",
    "\n",
    "print(f\"Total rows with coordinates: {total_with_coords:,}\")\n",
    "print(f\"Coordinates within Bangkok bounds: {valid_bangkok:,} ({valid_bangkok/total_with_coords*100:.1f}%)\")\n",
    "print(f\"Coordinate outliers (outside Bangkok): {outliers:,} ({outliers/total_with_coords*100:.1f}%)\")\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before - all coordinates\n",
    "sample_all = df_clean[df_clean['has_valid_coords'] == 1].sample(min(5000, valid_bangkok), random_state=42)\n",
    "axes[0].scatter(sample_all['longitude'], sample_all['latitude'], alpha=0.3, s=1)\n",
    "axes[0].axhline(y=BANGKOK_LAT_MIN, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=BANGKOK_LAT_MAX, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=BANGKOK_LON_MIN, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axvline(x=BANGKOK_LON_MAX, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title('All Coordinates (with Bangkok bounds)')\n",
    "axes[0].set_xlabel('Longitude')\n",
    "axes[0].set_ylabel('Latitude')\n",
    "\n",
    "# After - only valid Bangkok coordinates\n",
    "sample_valid = df_clean[df_clean['coords_in_bangkok'] == 1].sample(min(5000, valid_bangkok), random_state=42)\n",
    "axes[1].scatter(sample_valid['longitude'], sample_valid['latitude'], alpha=0.3, s=1, color='green')\n",
    "axes[1].set_title('Valid Bangkok Coordinates Only')\n",
    "axes[1].set_xlabel('Longitude')\n",
    "axes[1].set_ylabel('Latitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING: Clean Thai Text\n",
    "print(\"=\"*60)\n",
    "print(\"DATA CLEANING: THAI TEXT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def clean_thai_text(text):\n",
    "    \"\"\"Clean and normalize Thai text\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Normalize Thai text using pythainlp\n",
    "    text = thai_normalize(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to comments (sample first to test, then full)\n",
    "print(\"Cleaning comment text...\")\n",
    "df_clean['comment_cleaned'] = df_clean['comment'].apply(clean_thai_text)\n",
    "\n",
    "# Clean the primary category (first category in the list)\n",
    "df_clean['primary_category'] = df_clean['type_list'].apply(lambda x: x[0] if len(x) > 0 else 'Unknown')\n",
    "\n",
    "# Report\n",
    "print(f\"Comment cleaning complete!\")\n",
    "print(f\"\\nSample cleaned comments:\")\n",
    "sample = df_clean[df_clean['comment_cleaned'] != ''].sample(3, random_state=42)\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"  Original: {row['comment'][:100]}...\")\n",
    "    print(f\"  Cleaned:  {row['comment_cleaned'][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING: Clean Categories & State\n",
    "print(\"=\"*60)\n",
    "print(\"DATA CLEANING: CATEGORIES & STATE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Standardize state values\n",
    "state_mapping = {\n",
    "    'เสร็จสิ้น': 'completed',\n",
    "    'กำลังดำเนินการ': 'in_progress', \n",
    "    'รอรับเรื่อง': 'pending'\n",
    "}\n",
    "df_clean['state_en'] = df_clean['state'].map(state_mapping).fillna('other')\n",
    "\n",
    "# Create is_resolved binary flag\n",
    "df_clean['is_resolved'] = (df_clean['state_en'] == 'completed').astype(int)\n",
    "\n",
    "# Report category cleaning\n",
    "print(\"State value standardization:\")\n",
    "print(df_clean['state_en'].value_counts())\n",
    "\n",
    "print(\"\\n\\nTop 10 Primary Categories:\")\n",
    "print(df_clean['primary_category'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING: Create ML-Ready Features\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Time-based features (already created: year, month, day_of_week, hour)\n",
    "df_clean['is_weekend'] = (df_clean['day_of_week'] >= 5).astype(int)\n",
    "df_clean['is_business_hours'] = ((df_clean['hour'] >= 8) & (df_clean['hour'] <= 17)).astype(int)\n",
    "\n",
    "# 2. Resolution time features\n",
    "df_clean['resolution_time_days'] = df_clean['resolution_time_hours'] / 24\n",
    "df_clean['quick_resolution'] = (df_clean['resolution_time_hours'] <= 24).astype(int)  # resolved within 24h\n",
    "\n",
    "# 3. Text features\n",
    "df_clean['has_comment'] = (df_clean['comment_length'] > 0).astype(int)\n",
    "df_clean['long_comment'] = (df_clean['comment_length'] > df_clean['comment_length'].median()).astype(int)\n",
    "\n",
    "# 4. Engagement features\n",
    "df_clean['was_reopened'] = (df_clean['count_reopen'] > 0).astype(int)\n",
    "df_clean['high_star'] = (df_clean['star'] >= 4).astype(int)\n",
    "\n",
    "# Summary of new features\n",
    "new_features = [\n",
    "    'has_photo', 'has_photo_after', 'has_valid_coords', 'coords_in_bangkok',\n",
    "    'is_weekend', 'is_business_hours', 'resolution_time_days', 'quick_resolution',\n",
    "    'has_comment', 'long_comment', 'was_reopened', 'high_star', \n",
    "    'is_resolved', 'primary_category', 'state_en', 'comment_cleaned'\n",
    "]\n",
    "\n",
    "print(f\"New features created for ML: {len(new_features)}\")\n",
    "for feat in new_features:\n",
    "    if feat in df_clean.columns:\n",
    "        if df_clean[feat].dtype in ['int64', 'float64', 'int32', 'float32']:\n",
    "            print(f\"  - {feat}: {df_clean[feat].dtype} (mean: {df_clean[feat].mean():.3f})\")\n",
    "        else:\n",
    "            print(f\"  - {feat}: {df_clean[feat].dtype} ({df_clean[feat].nunique()} unique values)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT: Save Cleaned Dataset\n",
    "print(\"=\"*60)\n",
    "print(\"EXPORTING CLEANED DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select columns for export (drop intermediate/redundant columns)\n",
    "columns_to_drop = ['photo', 'photo_after', 'coords', 'type', 'comment']  # Keep cleaned versions\n",
    "df_export = df_clean.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Also need to handle the type_list column (can't save list to CSV directly)\n",
    "df_export['categories'] = df_export['type_list'].apply(lambda x: '|'.join(x) if isinstance(x, list) else '')\n",
    "df_export = df_export.drop(columns=['type_list'], errors='ignore')\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'bangkok_traffy_cleaned.csv'\n",
    "df_export.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {output_path}\")\n",
    "print(f\"Shape: {df_export.shape[0]:,} rows × {df_export.shape[1]} columns\")\n",
    "print(f\"File size: {df_export.memory_usage(deep=True).sum() / 1024**2:.2f} MB (in memory)\")\n",
    "\n",
    "# Show final columns\n",
    "print(f\"\\nFinal columns ({len(df_export.columns)}):\")\n",
    "for i, col in enumerate(df_export.columns, 1):\n",
    "    print(f\"  {i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA QUALITY REPORT\n",
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY REPORT - READY FOR ML\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total records: {len(df_export):,}\")\n",
    "print(f\"Total features: {len(df_export.columns)}\")\n",
    "print(f\"Date range: {df_export['timestamp'].min()} to {df_export['timestamp'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES (Final)\")\n",
    "print(\"=\"*60)\n",
    "final_missing = df_export.isnull().sum()\n",
    "if final_missing.sum() > 0:\n",
    "    print(final_missing[final_missing > 0])\n",
    "else:\n",
    "    print(\"No missing values in the cleaned dataset!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Numeric features: {len(df_export.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"Categorical features: {len(df_export.select_dtypes(include=['object']).columns)}\")\n",
    "print(f\"Datetime features: {len(df_export.select_dtypes(include=['datetime64']).columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML-READY FEATURES\")\n",
    "print(\"=\"*60)\n",
    "binary_features = ['has_photo', 'has_photo_after', 'has_valid_coords', 'coords_in_bangkok',\n",
    "                   'is_weekend', 'is_business_hours', 'quick_resolution', 'has_comment',\n",
    "                   'long_comment', 'was_reopened', 'high_star', 'is_resolved']\n",
    "numeric_features = ['star', 'count_reopen', 'num_categories', 'comment_length', \n",
    "                    'latitude', 'longitude', 'year', 'month', 'day_of_week', 'hour',\n",
    "                    'resolution_time_hours', 'resolution_time_days', 'comment_word_count']\n",
    "categorical_features = ['district', 'subdistrict', 'organization', 'state_en', 'primary_category']\n",
    "\n",
    "print(f\"\\nBinary features ({len(binary_features)}): {', '.join(binary_features)}\")\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}): {', '.join(numeric_features)}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {', '.join(categorical_features)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"READY FOR ML CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "checklist = [\n",
    "    (\"Missing values handled\", final_missing.sum() == 0),\n",
    "    (\"Timestamps converted to datetime\", df_export['timestamp'].dtype == 'datetime64[ns]'),\n",
    "    (\"Coordinates parsed and validated\", 'latitude' in df_export.columns and 'coords_in_bangkok' in df_export.columns),\n",
    "    (\"Thai text normalized\", 'comment_cleaned' in df_export.columns),\n",
    "    (\"Categories parsed\", 'primary_category' in df_export.columns),\n",
    "    (\"Binary features created\", all(f in df_export.columns for f in binary_features)),\n",
    "    (\"Resolution time calculated\", 'resolution_time_hours' in df_export.columns),\n",
    "    (\"State standardized to English\", 'state_en' in df_export.columns)\n",
    "]\n",
    "\n",
    "for item, passed in checklist:\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    print(f\"  [{status}] {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCleaned dataset saved as: bangkok_traffy_cleaned.csv\")\n",
    "print(\"Ready for machine learning modeling!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
